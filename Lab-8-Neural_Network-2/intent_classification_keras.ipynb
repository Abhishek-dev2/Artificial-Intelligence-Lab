{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(file):\n",
    "    data = []\n",
    "    temp = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            text = line.split('\\t')\n",
    "            word = text[1][text[1].index('#')+2:]\n",
    "            if word == \"BOS\":\n",
    "                label = text[2][text[2].index('#') +2:]\n",
    "            elif word == 'EOS':\n",
    "                if '+' in label or label =='abbreviation':\n",
    "                    temp = []\n",
    "                    continue\n",
    "                else:\n",
    "                    data.append([temp, label])\n",
    "                    temp = []\n",
    "                    continue\n",
    "            else:\n",
    "                temp.append(word)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_sentence('./atis.train.ctf.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_sentence('./atis.test.ctf.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['i', 'want', 'to', 'fly', 'from', 'boston', 'at', '838', 'am', 'and', 'arrive', 'in', 'denver', 'at', '1110', 'in', 'the', 'morning'], 'flight'], [['what', 'flights', 'are', 'available', 'from', 'pittsburgh', 'to', 'baltimore', 'on', 'thursday', 'morning'], 'flight']]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_dict(train, test):\n",
    "    unique_words = set()\n",
    "    unique_label = set()\n",
    "    for dataset in [train, test]:\n",
    "        for line in dataset:\n",
    "            for word in line[0]:\n",
    "                unique_words.add(word)\n",
    "            unique_label.add(line[1])\n",
    "    return (unique_words, unique_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set, labels_set = get_unigram_dict(train_data, test_data)\n",
    "words_list = list(words_set)\n",
    "labels_list = list(labels_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['usa', 'dca', 'please', 'daily', 'route', 'economy', 'houston', 'bound', 'landings', 'must']\n",
      "['flight_no', 'city', 'distance', 'airfare', 'meal', 'cheapest', 'aircraft', 'ground_service', 'restriction', 'capacity', 'ground_fare', 'airline', 'day_name', 'flight', 'airport', 'quantity', 'flight_time']\n"
     ]
    }
   ],
   "source": [
    "print(words_list[:10])\n",
    "print(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898\n"
     ]
    }
   ],
   "source": [
    "print(len(words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data, words_list, labels_list):\n",
    "    X=[]\n",
    "    y = []\n",
    "    for line in data:\n",
    "        temp =[0]*len(words_list)\n",
    "        for i, word in enumerate(line[0]):\n",
    "            temp[words_list.index(word)]+=1\n",
    "        X.append(temp)\n",
    "        temp_y = [0]*len(labels_list)\n",
    "        temp_y[labels_list.index(line[1])]+=1\n",
    "        y.append(temp_y)\n",
    "\n",
    "    return(X, y)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocessing(train_data, words_list, labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "print(len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = preprocessing(test_data, words_list, labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4805/4805 [==============================] - 1s 221us/step - loss: 1.1078 - acc: 0.7617\n",
      "Epoch 2/20\n",
      "4805/4805 [==============================] - 1s 106us/step - loss: 0.4098 - acc: 0.8974\n",
      "Epoch 3/20\n",
      "4805/4805 [==============================] - 0s 103us/step - loss: 0.2339 - acc: 0.9386\n",
      "Epoch 4/20\n",
      "4805/4805 [==============================] - 1s 104us/step - loss: 0.1552 - acc: 0.9586\n",
      "Epoch 5/20\n",
      "4805/4805 [==============================] - 1s 105us/step - loss: 0.1078 - acc: 0.9742\n",
      "Epoch 6/20\n",
      "4805/4805 [==============================] - 1s 118us/step - loss: 0.0755 - acc: 0.9840\n",
      "Epoch 7/20\n",
      "4805/4805 [==============================] - 1s 119us/step - loss: 0.0543 - acc: 0.9900\n",
      "Epoch 8/20\n",
      "4805/4805 [==============================] - 1s 106us/step - loss: 0.0417 - acc: 0.9919\n",
      "Epoch 9/20\n",
      "4805/4805 [==============================] - 1s 106us/step - loss: 0.0351 - acc: 0.9950\n",
      "Epoch 10/20\n",
      "4805/4805 [==============================] - 1s 105us/step - loss: 0.0248 - acc: 0.9965\n",
      "Epoch 11/20\n",
      "4805/4805 [==============================] - 1s 107us/step - loss: 0.0189 - acc: 0.9975\n",
      "Epoch 12/20\n",
      "4805/4805 [==============================] - 1s 112us/step - loss: 0.0149 - acc: 0.9979\n",
      "Epoch 13/20\n",
      "4805/4805 [==============================] - 1s 132us/step - loss: 0.0117 - acc: 0.9983\n",
      "Epoch 14/20\n",
      "4805/4805 [==============================] - 1s 117us/step - loss: 0.0093 - acc: 0.9988\n",
      "Epoch 15/20\n",
      "4805/4805 [==============================] - 1s 112us/step - loss: 0.0077 - acc: 0.9990\n",
      "Epoch 16/20\n",
      "4805/4805 [==============================] - 1s 119us/step - loss: 0.0067 - acc: 0.9992 0s - loss: 0.0077 - acc: 0.\n",
      "Epoch 17/20\n",
      "4805/4805 [==============================] - 1s 110us/step - loss: 0.0051 - acc: 0.9996\n",
      "Epoch 18/20\n",
      "4805/4805 [==============================] - 1s 122us/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "4805/4805 [==============================] - 1s 112us/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "4805/4805 [==============================] - 1s 109us/step - loss: 0.0030 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x227ce4d05f8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim = len(words_list), activation='relu'))\n",
    "model.add(Dense(16, input_dim = 64, activation='relu'))\n",
    "model.add(Dense(len(labels_list), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs = 20, verbose =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_temp = model.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred =[]\n",
    "for line in pred_temp:\n",
    "    line = line.tolist()\n",
    "    temp = [0]*len(labels_list)\n",
    "    temp[line.index(max(line))]=1\n",
    "    pred.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9289940828402367\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(np.array(y_test), np.array(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6738049331290883, 0.5729309856298727, 0.5743886763225182)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\Miniconda3\\envs\\for_ml\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\ragha\\Miniconda3\\envs\\for_ml\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(precision_recall_fscore_support(np.array(y_test), np.array(pred), average='macro')[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['i', 'would', 'like', 'to', 'find', 'a', 'flight', 'from', 'charlotte', 'to', 'las', 'vegas', 'that', 'makes', 'a', 'stop', 'in', 'st.', 'louis'], 'flight'], [['on', 'april', 'first', 'i', 'need', 'a', 'ticket', 'from', 'tacoma', 'to', 'san', 'jose', 'departing', 'before', '7', 'am'], 'airfare'], [['on', 'april', 'first', 'i', 'need', 'a', 'flight', 'going', 'from', 'phoenix', 'to', 'san', 'diego'], 'flight'], [['i', 'would', 'like', 'a', 'flight', 'traveling', 'one', 'way', 'from', 'phoenix', 'to', 'san', 'diego', 'on', 'april', 'first'], 'flight']]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(test_data[:4])\n",
    "print(pred[:4])\n",
    "print(y_test[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:for_ml]",
   "language": "python",
   "name": "conda-env-for_ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
