{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import ngrams\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_headline_category(category_list):\n",
    "    headline_category = []\n",
    "    try:\n",
    "        input_file = open('./News_Category_Dataset.json')\n",
    "        input_data = input_file.readlines()\n",
    "        input_file.close()\n",
    "        for json_object in input_data:\n",
    "            data = json.loads(json_object)\n",
    "            category = data['category'].upper()\n",
    "            if category in category_list:\n",
    "                headline_category.append((data['headline'], data['category']))\n",
    "        return headline_category\n",
    "    except IOError:\n",
    "        print(\"ERROR : IO ERROR occurred while opening file\")\n",
    "        exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = ['Business', 'Comedy','Sports', 'Crime', 'Religion']\n",
    "category_list = [i.upper() for i in category_list]\n",
    "headlines_and_category = extract_headline_category(category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV', 'CRIME'), ('Rachel Dolezal Faces Felony Charges For Welfare Fraud', 'CRIME'), (\"Trump's New 'MAGA'-Themed Swimwear Sinks On Twitter\", 'COMEDY'), ('Seth Meyers Has 1 Funny Regret After Trump Cancels North Korea Summit', 'COMEDY'), ('Colbert Wants To Turn NYC Subway Rides Into A New And Terrible Punishment', 'COMEDY'), (\"Man Faces Charges After Pulling Knife, Stun Gun On Muslim Students At McDonald's\", 'CRIME'), (\"Jimmy Kimmel Knows Why Iran's Supreme Leader Watches 'Tom And Jerry'\", 'COMEDY'), ('2 People Injured In Indiana School Shooting', 'CRIME'), (\"'Late Night' Writer's Breathless Royal Wedding Recap Is The Only One You Need\", 'COMEDY'), (\"Jets Chairman Christopher Johnson Won't Fine Players For Anthem Protests\", 'SPORTS'), ('Colbert Exposes The\\xa0Biggest Flaw In Trumpâ€™s Latest Conspiracy Theory', 'COMEDY'), ('Seth Meyers Gives Donald Trump Some Valuable Marketing Advice', 'COMEDY'), ('U.S. Launches Auto Import Probe, China Vows To Defend Its Interests', 'BUSINESS'), ('Trump Posthumously Pardons Boxer Jack Johnson', 'SPORTS'), (\"Samantha Bee Torches ICE: 'Let's Shut It The F**k Down'\", 'COMEDY'), ('Anna Kournikova Dancing With Her Bouncing Baby Is The Cutest Thing', 'SPORTS'), (\"Trump Says NFL Players Unwilling To Stand For Anthem Maybe 'Shouldn't Be In The Country'\", 'SPORTS'), ('Southern Baptist Leader Who Said Abused Women Should Just Pray Is Removed From Post', 'RELIGION'), ('Brandi Chastain Totally Agrees Her Hall Of Fame Plaque Looks Nothing Like Her', 'SPORTS'), (\"'Late Night' Writer Vows To 'Take Red Hats Back' From Trump Supporters\", 'COMEDY')]\n"
     ]
    }
   ],
   "source": [
    "print(headlines_and_category[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17841\n"
     ]
    }
   ],
   "source": [
    "print(len(headlines_and_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_grams(dataset:'headline, category', n:'n gram value', k:'return top k n-grams'):\n",
    "    n_grams_list=[]\n",
    "    for headline, category in dataset:\n",
    "        tokenize = nltk.word_tokenize(headline)\n",
    "        n_gram = nltk.ngrams(tokenize, n)\n",
    "        n_grams_list.extend(n_gram)\n",
    "    print(len(n_grams_list))\n",
    "    top_k = Counter(n_grams_list).most_common(k)\n",
    "    most_frequent=[i[0] for i in top_k]\n",
    "    return most_frequent\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\Miniconda3\\envs\\for_ml\\lib\\site-packages\\ipykernel\\__main__.py:6: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169606\n",
      "151792\n"
     ]
    }
   ],
   "source": [
    "unigrams_dict=  get_n_grams(headlines_and_category, 1, 500)\n",
    "bigrams_dict = get_n_grams(headlines_and_category, 2, 300)\n",
    "trigrams_dict = get_n_grams(headlines_and_category, 3, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = list({'CD', 'CC', 'RP', 'NNPS', 'IN', ',', '$', 'FW', 'RBR', 'JJ', \"''\", ')', 'VBD', 'VBP', 'POS', ':', 'NNS', '#', 'PRP', '(', 'VBN', 'PDT', 'JJS', 'VBG', 'PRP$', 'RBS', 'LS', '.', 'EX', 'NN', '``', 'DT', 'RB', 'WDT', 'VB', 'UH', 'TO', 'JJR', 'VBZ', 'MD', 'NNP', 'WP', 'WRB'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(dataset, unigrams_dict, bigrams_dict, trigrams_dict, pos_list, category_list):\n",
    "    X = []\n",
    "    y = []\n",
    "    for headline, category in dataset:\n",
    "        text = nltk.word_tokenize(headline)\n",
    "        unigrams = nltk.ngrams(text, 1)\n",
    "        bigrams = nltk.ngrams(text, 2)\n",
    "        trigrams = nltk.ngrams(text, 3)\n",
    "        list_of_ngrams_dict = [unigrams_dict, bigrams_dict, trigrams_dict]\n",
    "        list_of_ngrams_sentences = [unigrams, bigrams, trigrams]\n",
    "        temp_sentence = []\n",
    "        for i, ngram in enumerate(list_of_ngrams_dict):\n",
    "            temp_ngram = [0]*len(ngram)\n",
    "            for word in list_of_ngrams_sentences[i]:\n",
    "                if word in ngram:\n",
    "                    temp_ngram[ngram.index(word)]+=1\n",
    "            temp_sentence.extend(temp_ngram)\n",
    "        \n",
    "    \n",
    "#         temp_pos = [0]*len(pos_list)\n",
    "#         for word, tag in nltk.pos_tag(text):\n",
    "#             temp_pos[pos_list.index(tag)]+=1\n",
    "#         temp_sentence.extend(temp_pos)\n",
    "        X.append(temp_sentence)\n",
    "        y.append(category_list.index(category))\n",
    "    return (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\Miniconda3\\envs\\for_ml\\lib\\site-packages\\ipykernel\\__main__.py:14: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_features(headlines_and_category, unigrams_dict, bigrams_dict, trigrams_dict, pos_list, category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61387801 0.58174692 0.64389698 0.57422969 0.58912556 0.59473094\n",
      " 0.58721256 0.53142536 0.53535354 0.50617284]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y, scoring = 'accuracy', cv = 10)\n",
    "print(scores)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5757772401644059\n"
     ]
    }
   ],
   "source": [
    "print(sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
